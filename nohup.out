/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-26 16:54:12,884] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:12,895] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:12,908] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:14,143] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-26 16:54:14,143] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=60012 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./dataset/converted-dpo_pairs.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1 --image_folder /home/yilin/yilin-DPO/dataset/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1 --project_name yilin-align --use_anchor True
[2025-10-26 16:54:14,168] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-26 16:54:14,168] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=60011 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./dataset/converted-dpo_pairs.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0 --image_folder /home/yilin/yilin-DPO/dataset/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0 --project_name yilin-align --use_anchor False
[2025-10-26 16:54:14,199] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-26 16:54:14,199] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=60010 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./dataset/converted-dpo_pairs.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0 --image_folder /home/yilin/yilin-DPO/dataset/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0 --project_name yilin-align --use_anchor True
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-26 16:54:16,256] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:16,295] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:16,340] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:17,110] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-26 16:54:17,110] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-26 16:54:17,110] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-26 16:54:17,110] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-26 16:54:17,110] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-26 16:54:17,172] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-26 16:54:17,172] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-26 16:54:17,172] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-26 16:54:17,172] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-26 16:54:17,172] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-26 16:54:17,212] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-26 16:54:17,212] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-26 16:54:17,212] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-26 16:54:17,212] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-26 16:54:17,212] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-26 16:54:22,176] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-26 16:54:22,470] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:22,543] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:54:22,554] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:22,571] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:54:22,629] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:54:22,683] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:54:22,863] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:54:22,863] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-26 16:54:22,929] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:54:22,933] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:54:23,009] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:54:23,009] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:54:23,066] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:54:23,067] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/yilin/yilin-DPO/wandb/run-20251026_165423-vr66vh8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/vr66vh8n
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/yilin/yilin-DPO/wandb/run-20251026_165423-845hr059
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/845hr059
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/yilin/yilin-DPO/wandb/run-20251026_165424-ivgod4eq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/ivgod4eq
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.89s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  3.00s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.80s/it]
to bfloat16...
Adding LoRA adapters...
Adding LoRA adapters...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.42s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.63s/it]seed: 1765543779, indices: [36639, 3845, 38073, 8852, 1415, 33423, 27306, 36089, 8149, 14864]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.90s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.71s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]
Formatting inputs...Skip in lazy mode
seed: 1778978272, indices: [22019, 13917, 13195, 14422, 47194, 31971, 46299, 32082, 14105, 10894]
Formatting inputs...Skip in lazy mode
seed: 2254221880, indices: [17592, 36532, 15826, 12690, 42292, 11200, 14084, 40201, 20378, 4564]
seed: 1796012521, indices: [42992, 16891, 2462, 40592, 45859, 42622, 9959, 19740, 18543, 40092]
seed: 2628124420, indices: [4375, 41997, 13734, 26010, 3482, 46459, 40394, 47534, 2771, 17391]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Formatting inputs...Skip in lazy mode
seed: 3916269600, indices: [4943, 39880, 7163, 27043, 40822, 50353, 18686, 24008, 37635, 29008]
[2025-10-26 16:55:11,247] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1889629
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mmm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251026_165423-845hr059/logs[0m
[2025-10-26 16:55:14,473] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1889630
[2025-10-26 16:55:14,474] [ERROR] [launch.py:321:sigkill_handler] ['/home/yilin/anaconda3/envs/re-align/bin/python3.10', '-u', 'train_rdpo.py', '--local_rank=1', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--data_path', './dataset/converted-dpo_pairs.json', '--deepspeed', './deepspeed/zero2.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-05', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--bf16', 'True', '--lora_enable', 'True', '--beta', '0.1', '--output_dir', '/home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1', '--image_folder', '/home/yilin/yilin-DPO/dataset/', '--mm_projector_lr', '2e-5', '--mm_projector_type', 'mlp2x_gelu', '--run_name', 'mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1', '--project_name', 'yilin-align', '--use_anchor', 'True'] exits with return code = 1
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mmm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251026_165424-ivgod4eq/logs[0m
[2025-10-26 16:55:18,198] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1889627
[2025-10-26 16:55:18,199] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1889628
[2025-10-26 16:55:18,227] [ERROR] [launch.py:321:sigkill_handler] ['/home/yilin/anaconda3/envs/re-align/bin/python3.10', '-u', 'train_rdpo.py', '--local_rank=1', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--data_path', './dataset/converted-dpo_pairs.json', '--deepspeed', './deepspeed/zero2.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-05', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--bf16', 'True', '--lora_enable', 'True', '--beta', '0.1', '--output_dir', '/home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0', '--image_folder', '/home/yilin/yilin-DPO/dataset/', '--mm_projector_lr', '2e-5', '--mm_projector_type', 'mlp2x_gelu', '--run_name', 'mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0', '--project_name', 'yilin-align', '--use_anchor', 'False'] exits with return code = 1
[2025-10-26 16:55:18,285] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1889631
[2025-10-26 16:55:18,314] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1889632
[2025-10-26 16:55:18,314] [ERROR] [launch.py:321:sigkill_handler] ['/home/yilin/anaconda3/envs/re-align/bin/python3.10', '-u', 'train_rdpo.py', '--local_rank=1', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--data_path', './dataset/converted-dpo_pairs.json', '--deepspeed', './deepspeed/zero2.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-05', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--bf16', 'True', '--lora_enable', 'True', '--beta', '0.1', '--output_dir', '/home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0', '--image_folder', '/home/yilin/yilin-DPO/dataset/', '--mm_projector_lr', '2e-5', '--mm_projector_type', 'mlp2x_gelu', '--run_name', 'mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0', '--project_name', 'yilin-align', '--use_anchor', 'True'] exits with return code = 1
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-26 16:55:25,306] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:25,314] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:25,325] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:26,588] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-26 16:55:26,588] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=60010 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./dataset/converted-dpo_pairs.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1 --image_folder /home/yilin/yilin-DPO/dataset/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1 --project_name yilin-align --use_anchor False
[2025-10-26 16:55:26,640] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-26 16:55:26,641] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=60011 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./dataset/converted-dpo_pairs.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2 --image_folder /home/yilin/yilin-DPO/dataset/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2 --project_name yilin-align --use_anchor True
[2025-10-26 16:55:26,656] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-26 16:55:26,656] [INFO] [runner.py:571:main] cmd = /home/yilin/anaconda3/envs/re-align/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=60012 --enable_each_rank_log=None train_rdpo.py --model_name_or_path liuhaotian/llava-v1.5-7b --data_path ./dataset/converted-dpo_pairs.json --deepspeed ./deepspeed/zero2.json --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --learning_rate 1e-05 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --bf16 True --lora_enable True --beta 0.1 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2 --image_folder /home/yilin/yilin-DPO/dataset/ --mm_projector_lr 2e-5 --mm_projector_type mlp2x_gelu --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2 --project_name yilin-align --use_anchor False
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-26 16:55:28,716] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:28,786] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:28,810] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:29,566] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-26 16:55:29,566] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-26 16:55:29,566] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-26 16:55:29,566] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-26 16:55:29,566] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-26 16:55:29,648] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-26 16:55:29,648] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-26 16:55:29,648] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-26 16:55:29,648] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-26 16:55:29,648] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-26 16:55:29,699] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-26 16:55:29,699] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-26 16:55:29,699] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-26 16:55:29,699] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-26 16:55:29,699] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-26 16:55:34,824] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:34,977] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-26 16:55:35,076] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:35,094] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:35,112] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-26 16:55:35,132] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-26 16:55:35,226] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
[2025-10-26 16:55:35,371] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:55:35,371] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Appending key for api.wandb.ai to your netrc file: /home/yilin/.netrc
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:55:35,456] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:55:35,493] [INFO] [comm.py:637:init_distributed] cdb=None
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: julyang5216 (julyang5216-yilin) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[2025-10-26 16:55:35,520] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:55:35,520] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-26 16:55:35,536] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-26 16:55:35,536] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/yilin/yilin-DPO/wandb/run-20251026_165536-gerezzlr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/gerezzlr
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/yilin/yilin-DPO/wandb/run-20251026_165536-a01xcmhm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/a01xcmhm
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/yilin/yilin-DPO/wandb/run-20251026_165536-bww7jkqb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2
wandb: ⭐️ View project at https://wandb.ai/julyang5216-yilin/yilin-align
wandb: 🚀 View run at https://wandb.ai/julyang5216-yilin/yilin-align/runs/bww7jkqb
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.16s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]
to bfloat16...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]
to bfloat16...
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.43s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.23s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
to bfloat16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]
to bfloat16...
to bfloat16...
Adding LoRA adapters...
Adding LoRA adapters...
Adding LoRA adapters...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.06s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.75s/it]
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.48s/it]You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
seed: 4193533803, indices: [42014, 36840, 32905, 8341, 19198, 38994, 17432, 4416, 13342, 24858]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Formatting inputs...Skip in lazy mode
seed: 1029260420, indices: [16238, 27105, 8916, 38830, 12253, 46630, 4547, 43712, 34247, 18251]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.98s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.61s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
seed: 2626939732, indices: [41741, 33458, 15710, 26510, 29575, 46891, 26258, 30355, 35947, 990]
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mmm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251026_165536-bww7jkqb/logs[0m
Formatting inputs...Skip in lazy mode
seed: 1698615064, indices: [25953, 45239, 45426, 35086, 5393, 30499, 28756, 32185, 2613, 31792]
seed: 407190202, indices: [26291, 47514, 17715, 16942, 4687, 2289, 14344, 2413, 40318, 10957]
Formatting inputs...Skip in lazy mode
seed: 579285498, indices: [36570, 10143, 12905, 43672, 48586, 7596, 40019, 27778, 8730, 19259]
[2025-10-26 16:56:25,653] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1891506
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:1')
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
[MyDPOTrainer] 自定义参数 text_similarity_mean = tensor([0.8500], device='cuda:0')
[2025-10-26 16:56:26,719] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1891508
[2025-10-26 16:56:26,720] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1891509
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
Traceback (most recent call last):
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1276, in <module>
    train()
  File "/home/yilin/yilin-DPO/train_rdpo.py", line 1232, in train
    trainer.train()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/yilin/anaconda3/envs/re-align/lib/python3.10/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/home/yilin/yilin-DPO/llava/train/llava_trainer.py", line 289, in _get_train_sampler
    lengths = self.train_dataset.modality_lengths
AttributeError: 'Subset' object has no attribute 'modality_lengths'
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mmm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251026_165536-a01xcmhm/logs[0m
[2025-10-26 16:56:28,333] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1891507
[2025-10-26 16:56:28,333] [ERROR] [launch.py:321:sigkill_handler] ['/home/yilin/anaconda3/envs/re-align/bin/python3.10', '-u', 'train_rdpo.py', '--local_rank=1', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--data_path', './dataset/converted-dpo_pairs.json', '--deepspeed', './deepspeed/zero2.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-05', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--bf16', 'True', '--lora_enable', 'True', '--beta', '0.1', '--output_dir', '/home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1', '--image_folder', '/home/yilin/yilin-DPO/dataset/', '--mm_projector_lr', '2e-5', '--mm_projector_type', 'mlp2x_gelu', '--run_name', 'mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1', '--project_name', 'yilin-align', '--use_anchor', 'False'] exits with return code = 1
[2025-10-26 16:56:29,390] [ERROR] [launch.py:321:sigkill_handler] ['/home/yilin/anaconda3/envs/re-align/bin/python3.10', '-u', 'train_rdpo.py', '--local_rank=1', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--data_path', './dataset/converted-dpo_pairs.json', '--deepspeed', './deepspeed/zero2.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-05', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--bf16', 'True', '--lora_enable', 'True', '--beta', '0.1', '--output_dir', '/home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2', '--image_folder', '/home/yilin/yilin-DPO/dataset/', '--mm_projector_lr', '2e-5', '--mm_projector_type', 'mlp2x_gelu', '--run_name', 'mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2', '--project_name', 'yilin-align', '--use_anchor', 'False'] exits with return code = 1
[2025-10-26 16:56:30,779] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1891510
[2025-10-26 16:56:30,808] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1891511
[2025-10-26 16:56:30,808] [ERROR] [launch.py:321:sigkill_handler] ['/home/yilin/anaconda3/envs/re-align/bin/python3.10', '-u', 'train_rdpo.py', '--local_rank=1', '--model_name_or_path', 'liuhaotian/llava-v1.5-7b', '--data_path', './dataset/converted-dpo_pairs.json', '--deepspeed', './deepspeed/zero2.json', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '8', '--evaluation_strategy', 'no', '--save_strategy', 'no', '--learning_rate', '1e-05', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--bf16', 'True', '--lora_enable', 'True', '--beta', '0.1', '--output_dir', '/home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2', '--image_folder', '/home/yilin/yilin-DPO/dataset/', '--mm_projector_lr', '2e-5', '--mm_projector_type', 'mlp2x_gelu', '--run_name', 'mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2', '--project_name', 'yilin-align', '--use_anchor', 'True'] exits with return code = 1
🚀 Running llava-v1.5-7b | lr=1e-05, bs=8
当前进程数: 1
🚀 Running llava-v1.5-7b | lr=1e-05, bs=8
当前进程数: 2
🚀 Running llava-v1.5-7b | lr=1e-05, bs=8
当前进程数: 3
❌ Failed: llava-v1.5-7b cmd=['deepspeed --include=localhost:0,1  --master_port 60010 train_rdpo.py                 --model_name_or_path liuhaotian/llava-v1.5-7b                 --data_path ./dataset/converted-dpo_pairs.json                 --deepspeed "./deepspeed/zero2.json"                 --per_device_train_batch_size 1                 --per_device_eval_batch_size 1                 --gradient_accumulation_steps 8                 --evaluation_strategy "no"                 --save_strategy "no"                 --learning_rate 1e-05                 --weight_decay 0.                 --warmup_ratio 0.03                 --lr_scheduler_type "cosine"                 --bf16 True                 --lora_enable True                 --beta 0.1                 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0                 --image_folder /home/yilin/yilin-DPO/dataset/                 --mm_projector_lr 2e-5                 --mm_projector_type mlp2x_gelu                 --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-0                 --project_name "yilin-align"                 --use_anchor True ']
❌ Failed: llava-v1.5-7b cmd=['deepspeed --include=localhost:0,1  --master_port 60011 train_rdpo.py                 --model_name_or_path liuhaotian/llava-v1.5-7b                 --data_path ./dataset/converted-dpo_pairs.json                 --deepspeed "./deepspeed/zero2.json"                 --per_device_train_batch_size 1                 --per_device_eval_batch_size 1                 --gradient_accumulation_steps 8                 --evaluation_strategy "no"                 --save_strategy "no"                 --learning_rate 1e-05                 --weight_decay 0.                 --warmup_ratio 0.03                 --lr_scheduler_type "cosine"                 --bf16 True                 --lora_enable True                 --beta 0.1                 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0                 --image_folder /home/yilin/yilin-DPO/dataset/                 --mm_projector_lr 2e-5                 --mm_projector_type mlp2x_gelu                 --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-0                 --project_name "yilin-align"                 --use_anchor False ']
❌ Failed: llava-v1.5-7b cmd=['deepspeed --include=localhost:0,1  --master_port 60012 train_rdpo.py                 --model_name_or_path liuhaotian/llava-v1.5-7b                 --data_path ./dataset/converted-dpo_pairs.json                 --deepspeed "./deepspeed/zero2.json"                 --per_device_train_batch_size 1                 --per_device_eval_batch_size 1                 --gradient_accumulation_steps 8                 --evaluation_strategy "no"                 --save_strategy "no"                 --learning_rate 1e-05                 --weight_decay 0.                 --warmup_ratio 0.03                 --lr_scheduler_type "cosine"                 --bf16 True                 --lora_enable True                 --beta 0.1                 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1                 --image_folder /home/yilin/yilin-DPO/dataset/                 --mm_projector_lr 2e-5                 --mm_projector_type mlp2x_gelu                 --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-1                 --project_name "yilin-align"                 --use_anchor True ']
🚀 Running llava-v1.5-7b | lr=1e-05, bs=8
当前进程数: 1
🚀 Running llava-v1.5-7b | lr=1e-05, bs=8
当前进程数: 2
🚀 Running llava-v1.5-7b | lr=1e-05, bs=8
当前进程数: 3
❌ Failed: llava-v1.5-7b cmd=['deepspeed --include=localhost:0,1  --master_port 60010 train_rdpo.py                 --model_name_or_path liuhaotian/llava-v1.5-7b                 --data_path ./dataset/converted-dpo_pairs.json                 --deepspeed "./deepspeed/zero2.json"                 --per_device_train_batch_size 1                 --per_device_eval_batch_size 1                 --gradient_accumulation_steps 8                 --evaluation_strategy "no"                 --save_strategy "no"                 --learning_rate 1e-05                 --weight_decay 0.                 --warmup_ratio 0.03                 --lr_scheduler_type "cosine"                 --bf16 True                 --lora_enable True                 --beta 0.1                 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1                 --image_folder /home/yilin/yilin-DPO/dataset/                 --mm_projector_lr 2e-5                 --mm_projector_type mlp2x_gelu                 --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-1                 --project_name "yilin-align"                 --use_anchor False ']
❌ Failed: llava-v1.5-7b cmd=['deepspeed --include=localhost:0,1  --master_port 60011 train_rdpo.py                 --model_name_or_path liuhaotian/llava-v1.5-7b                 --data_path ./dataset/converted-dpo_pairs.json                 --deepspeed "./deepspeed/zero2.json"                 --per_device_train_batch_size 1                 --per_device_eval_batch_size 1                 --gradient_accumulation_steps 8                 --evaluation_strategy "no"                 --save_strategy "no"                 --learning_rate 1e-05                 --weight_decay 0.                 --warmup_ratio 0.03                 --lr_scheduler_type "cosine"                 --bf16 True                 --lora_enable True                 --beta 0.1                 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2                 --image_folder /home/yilin/yilin-DPO/dataset/                 --mm_projector_lr 2e-5                 --mm_projector_type mlp2x_gelu                 --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-True-2                 --project_name "yilin-align"                 --use_anchor True ']
❌ Failed: llava-v1.5-7b cmd=['deepspeed --include=localhost:0,1  --master_port 60012 train_rdpo.py                 --model_name_or_path liuhaotian/llava-v1.5-7b                 --data_path ./dataset/converted-dpo_pairs.json                 --deepspeed "./deepspeed/zero2.json"                 --per_device_train_batch_size 1                 --per_device_eval_batch_size 1                 --gradient_accumulation_steps 8                 --evaluation_strategy "no"                 --save_strategy "no"                 --learning_rate 1e-05                 --weight_decay 0.                 --warmup_ratio 0.03                 --lr_scheduler_type "cosine"                 --bf16 True                 --lora_enable True                 --beta 0.1                 --output_dir /home/yilin/yilin-DPO/output/llava-v1.5-7b/mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2                 --image_folder /home/yilin/yilin-DPO/dataset/                 --mm_projector_lr 2e-5                 --mm_projector_type mlp2x_gelu                 --run_name mm_dpo-lr-1e-05-acc_batch-8-beta-0.1-use_anchor-False-2                 --project_name "yilin-align"                 --use_anchor False ']
